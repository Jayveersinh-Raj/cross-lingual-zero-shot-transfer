{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Checking for GPU"
      ],
      "metadata": {
        "id": "2zr8KGIBwYQB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "dyB7CxkAHZuo",
        "outputId": "5e08db8a-95a1-4116-d037-8dc5f78b8370",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Mar  7 13:58:32 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   70C    P0    28W /  70W |   8547MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "isGp1vtyHL7E",
        "outputId": "578355fc-21f5-4cda-9134-26642141f02b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EaYJhJl-HN_V",
        "outputId": "62eee59d-28a7-4605-d406-fb835bc8c44e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installations"
      ],
      "metadata": {
        "id": "WjYylQRkweFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pvr61-5DIjWF",
        "outputId": "18e660d8-314d-42fb-ba6f-00feddcc57b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.8/dist-packages (0.1.97)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbRHDRZaHQkv",
        "outputId": "d2877c80-14a3-49ac-9b3e-c2f2acb60716"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.26.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.26.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reading the csv file, and merging the subcategories"
      ],
      "metadata": {
        "id": "6q5vEL8kwv68"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "aVgujlKKHiuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/cleaned_data (1).csv')"
      ],
      "metadata": {
        "id": "2706rUGtBGtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['toxicity'] = data[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].max(axis=1)"
      ],
      "metadata": {
        "id": "_NAj3XJnVQMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.drop(columns=['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate','id','comment_text','Unnamed: 0'], inplace=True)"
      ],
      "metadata": {
        "id": "lNjVRT4yWf8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.rename(columns={ 'toxic': 'lebel'}, inplace=True)"
      ],
      "metadata": {
        "id": "KeXYPOJOW8Js"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.drop(columns=['Unnamed: 0'], inplace=True)"
      ],
      "metadata": {
        "id": "8lntYEeHBwcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drop rows with NaN values\n",
        "data.dropna(axis=0, inplace=True)"
      ],
      "metadata": {
        "id": "5UgeJ0_NebJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "L6m8U0XnX0WZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "e6ef7baa-8872-499a-b291-3aaa5c24053c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                     cleaned_comment  toxicity\n",
              "0  explanation why edits made username hardcore m...         0\n",
              "1  he matches background colour i seemingly stuck...         0\n",
              "2  hey man i really trying edit war it guy consta...         0\n",
              "3  more i ca make real suggestions improvement i ...         0\n",
              "4              you sir hero any chance remember page         0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6daa46f1-0b4a-4904-b01f-50ba9003ca9f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cleaned_comment</th>\n",
              "      <th>toxicity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>explanation why edits made username hardcore m...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>he matches background colour i seemingly stuck...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>hey man i really trying edit war it guy consta...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>more i ca make real suggestions improvement i ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>you sir hero any chance remember page</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6daa46f1-0b4a-4904-b01f-50ba9003ca9f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6daa46f1-0b4a-4904-b01f-50ba9003ca9f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6daa46f1-0b4a-4904-b01f-50ba9003ca9f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing dependencies and XLM-Roberta for tokenization"
      ],
      "metadata": {
        "id": "kM6pwCycw-_c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification, AdamW \n",
        "from transformers import XLMRobertaTokenizer, XLMRobertaModel"
      ],
      "metadata": {
        "id": "7s_Iv9n3W4SW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the XLM-Roberta tokenizer\n",
        "print('Loading XLMRobertaTokenizer ...')\n",
        "tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base', do_lower_case=True)\n",
        "\n",
        "# Load the XLM-Roberta model\n",
        "print('Loading XLMRobertaModel ...')\n",
        "model = XLMRobertaModel.from_pretrained('xlm-roberta-base')"
      ],
      "metadata": {
        "id": "Op6yw23yW4dn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6f82d6f-e5fb-4021-ae30-1def23b928a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading XLMRobertaTokenizer ...\n",
            "Loading XLMRobertaModel ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentences and labels"
      ],
      "metadata": {
        "id": "ocEy-pL0t2Rq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = data.cleaned_comment.values\n",
        "labels = data.toxicity.values"
      ],
      "metadata": {
        "id": "Ln1gco08aKea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizing"
      ],
      "metadata": {
        "id": "xd-DLoi_t8yt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # encode_plus will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the [CLS] token to the start.\n",
        "    #   (3) Append the [SEP] token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    #   (5) Pad or truncate the sentence to max_length\n",
        "    #   (6) Create attention masks for [PAD] tokens.\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 128,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', sentences[0])\n",
        "print('Token IDs:', input_ids[0])\n",
        "print('labels:', labels)"
      ],
      "metadata": {
        "id": "YGR1fINBZ88G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "067f4390-0f2e-491e-8f94-fc2f7398ec2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:  explanation why edits made username hardcore metallica fan reverted they vandalisms closure gas i voted new york dolls fac and please remove template talk page since i retired\n",
            "Token IDs: tensor([     0, 187136,  15400,  27211,      7,   7228,  38937,  11627,  24041,\n",
            "         99665,    408,   1207,  39531,   3674,   1836,  19521,  10836,   4432,\n",
            "             6, 170224,   9060,     17,  43374,     71,   3525,  70662,     92,\n",
            "            54,  42458,   7808,    136,  22936,  87388, 110934,  22120,   9191,\n",
            "         16792,     17,  90223,     71,      2,      1,      1,      1,      1,\n",
            "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
            "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
            "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
            "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
            "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
            "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
            "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
            "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
            "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
            "             1,      1])\n",
            "labels: tensor([0, 0, 0,  ..., 0, 0, 0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Having a look"
      ],
      "metadata": {
        "id": "uG12GavBt_oJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Original: ', sentences[0])\n",
        "print('Token IDs:', input_ids[0])\n",
        "print('labels:', labels)"
      ],
      "metadata": {
        "id": "N56e_KuMd2qq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ade3096c-56a8-451c-a342-a3a6b251a6ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:  explanation why edits made username hardcore metallica fan reverted they vandalisms closure gas i voted new york dolls fac and please remove template talk page since i retired\n",
            "Token IDs: tensor([     0, 187136,  15400,  27211,      7,   7228,  38937,  11627,  24041,\n",
            "         99665,    408,   1207,  39531,   3674,   1836,  19521,  10836,   4432,\n",
            "             6, 170224,   9060,     17,  43374,     71,   3525,  70662,     92,\n",
            "            54,  42458,   7808,    136,  22936,  87388, 110934,  22120,   9191,\n",
            "         16792,     17,  90223,     71,      2,      1,      1,      1,      1,\n",
            "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
            "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
            "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
            "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
            "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
            "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
            "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
            "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
            "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
            "             1,      1])\n",
            "labels: tensor([0, 0, 0,  ..., 0, 0, 0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Test split"
      ],
      "metadata": {
        "id": "VMtUrwWWgJNp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, random_split\n",
        "\n",
        "# Combine the training inputs into a TensorDataset.\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "# Create a 80-20 train-validation split.\n",
        "\n",
        "# Calculate the number of samples to include in each set.\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "# Divide the dataset by randomly selecting samples.\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "print('{:>5,} training samples'.format(train_size))\n",
        "print('{:>5,} validation samples'.format(val_size))"
      ],
      "metadata": {
        "id": "RVskAhMlc4lb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcba21e9-eb92-41dd-9d77-6235c15fe939"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "127,578 training samples\n",
            "31,895 validation samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Datatloaders for training and validation"
      ],
      "metadata": {
        "id": "HVbn1CPYuIIC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it \n",
        "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
        "# size of 16 or 32.\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoaders for our training and validation sets.\n",
        "# We'll take training samples in random order. \n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  # The training samples.\n",
        "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )\n",
        "\n",
        "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset, # The validation samples.\n",
        "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
        "            batch_size = batch_size # Evaluate with this batch size.\n",
        "        )\n",
        "     "
      ],
      "metadata": {
        "id": "Gjin5ANnJmVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XLM-Roberta Sequence Classifier"
      ],
      "metadata": {
        "id": "ebhiAlHluOQG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import XLMRobertaForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "# linear classification layer on top. \n",
        "model = XLMRobertaForSequenceClassification.from_pretrained(\n",
        "    \"xlm-roberta-base\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
        "                    # You can increase this for multi-class tasks.   \n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model.cuda()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0oTHRT59Jo2g",
        "outputId": "4afb50b8-58a3-44d1-f230-e9a6eec7741e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XLMRobertaForSequenceClassification(\n",
              "  (roberta): XLMRobertaModel(\n",
              "    (embeddings): XLMRobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): XLMRobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): XLMRobertaLayer(\n",
              "          (attention): XLMRobertaAttention(\n",
              "            (self): XLMRobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): XLMRobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): XLMRobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): XLMRobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): XLMRobertaLayer(\n",
              "          (attention): XLMRobertaAttention(\n",
              "            (self): XLMRobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): XLMRobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): XLMRobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): XLMRobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): XLMRobertaLayer(\n",
              "          (attention): XLMRobertaAttention(\n",
              "            (self): XLMRobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): XLMRobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): XLMRobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): XLMRobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): XLMRobertaLayer(\n",
              "          (attention): XLMRobertaAttention(\n",
              "            (self): XLMRobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): XLMRobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): XLMRobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): XLMRobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): XLMRobertaLayer(\n",
              "          (attention): XLMRobertaAttention(\n",
              "            (self): XLMRobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): XLMRobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): XLMRobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): XLMRobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): XLMRobertaLayer(\n",
              "          (attention): XLMRobertaAttention(\n",
              "            (self): XLMRobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): XLMRobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): XLMRobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): XLMRobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): XLMRobertaLayer(\n",
              "          (attention): XLMRobertaAttention(\n",
              "            (self): XLMRobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): XLMRobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): XLMRobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): XLMRobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): XLMRobertaLayer(\n",
              "          (attention): XLMRobertaAttention(\n",
              "            (self): XLMRobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): XLMRobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): XLMRobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): XLMRobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): XLMRobertaLayer(\n",
              "          (attention): XLMRobertaAttention(\n",
              "            (self): XLMRobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): XLMRobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): XLMRobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): XLMRobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): XLMRobertaLayer(\n",
              "          (attention): XLMRobertaAttention(\n",
              "            (self): XLMRobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): XLMRobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): XLMRobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): XLMRobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): XLMRobertaLayer(\n",
              "          (attention): XLMRobertaAttention(\n",
              "            (self): XLMRobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): XLMRobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): XLMRobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): XLMRobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): XLMRobertaLayer(\n",
              "          (attention): XLMRobertaAttention(\n",
              "            (self): XLMRobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): XLMRobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): XLMRobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): XLMRobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (classifier): XLMRobertaClassificationHead(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model view"
      ],
      "metadata": {
        "id": "ngUKzjJJuW42"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(model.named_parameters())\n",
        "\n",
        "print('The XLMRoberta model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "print('==== Embedding Layer ====\\n')\n",
        "\n",
        "for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "for p in params[5:21]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "for p in params[-4:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEy48JEyJ0ap",
        "outputId": "85cbd395-2d81-41b7-b779-b86274c52d6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The XLMRoberta model has 201 different named parameters.\n",
            "\n",
            "==== Embedding Layer ====\n",
            "\n",
            "roberta.embeddings.word_embeddings.weight               (250002, 768)\n",
            "roberta.embeddings.position_embeddings.weight             (514, 768)\n",
            "roberta.embeddings.token_type_embeddings.weight             (1, 768)\n",
            "roberta.embeddings.LayerNorm.weight                           (768,)\n",
            "roberta.embeddings.LayerNorm.bias                             (768,)\n",
            "\n",
            "==== First Transformer ====\n",
            "\n",
            "roberta.encoder.layer.0.attention.self.query.weight       (768, 768)\n",
            "roberta.encoder.layer.0.attention.self.query.bias             (768,)\n",
            "roberta.encoder.layer.0.attention.self.key.weight         (768, 768)\n",
            "roberta.encoder.layer.0.attention.self.key.bias               (768,)\n",
            "roberta.encoder.layer.0.attention.self.value.weight       (768, 768)\n",
            "roberta.encoder.layer.0.attention.self.value.bias             (768,)\n",
            "roberta.encoder.layer.0.attention.output.dense.weight     (768, 768)\n",
            "roberta.encoder.layer.0.attention.output.dense.bias           (768,)\n",
            "roberta.encoder.layer.0.attention.output.LayerNorm.weight       (768,)\n",
            "roberta.encoder.layer.0.attention.output.LayerNorm.bias       (768,)\n",
            "roberta.encoder.layer.0.intermediate.dense.weight        (3072, 768)\n",
            "roberta.encoder.layer.0.intermediate.dense.bias              (3072,)\n",
            "roberta.encoder.layer.0.output.dense.weight              (768, 3072)\n",
            "roberta.encoder.layer.0.output.dense.bias                     (768,)\n",
            "roberta.encoder.layer.0.output.LayerNorm.weight               (768,)\n",
            "roberta.encoder.layer.0.output.LayerNorm.bias                 (768,)\n",
            "\n",
            "==== Output Layer ====\n",
            "\n",
            "classifier.dense.weight                                   (768, 768)\n",
            "classifier.dense.bias                                         (768,)\n",
            "classifier.out_proj.weight                                  (2, 768)\n",
            "classifier.out_proj.bias                                        (2,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimizer (AdamW -> Huggingface)"
      ],
      "metadata": {
        "id": "6pWzZg6UuabR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
        "# I believe the 'W' stands for 'Weight Decay fix\"\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )"
      ],
      "metadata": {
        "id": "N7sIEnW8J2Nx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e672aead-cd25-4733-f4d9-8089cd58fb81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Epochs, training steps, and LR scheduler"
      ],
      "metadata": {
        "id": "M7UgPIeuuk1g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
        "# We chose to run for 3, but we'll see later that this may be over-fitting the\n",
        "# training data.\n",
        "epochs = 3\n",
        "\n",
        "# Total number of training steps is [number of batches] x [number of epochs]. \n",
        "# (Note that this is not the same as the number of training samples).\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)\n",
        "     "
      ],
      "metadata": {
        "id": "KodXCGppJ4Fj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accuracy function"
      ],
      "metadata": {
        "id": "dgtFz5bUuxcp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "metadata": {
        "id": "7GtEJELaJ7tW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Format time"
      ],
      "metadata": {
        "id": "whTX3X2tu07L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "metadata": {
        "id": "c4kGmk7HJ_HT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports of dependencies before training "
      ],
      "metadata": {
        "id": "aYejOlpVu35p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import datetime\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "from transformers import get_linear_schedule_with_warmup\n"
      ],
      "metadata": {
        "id": "iKX3XfiHOL6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "2_kw3GKxu9CH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# We'll store a number of quantities such as training and validation loss, \n",
        "# validation accuracy, and timings.\n",
        "training_stats = []\n",
        "\n",
        "# Measure the total training time for the whole run.\n",
        "total_t0 = time.time()\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        # It returns different numbers of parameters depending on what arguments\n",
        "        # arge given and what flags are set. For our useage here, it returns\n",
        "        # the loss (because we provided labels) and the \"logits\"--the model\n",
        "        # outputs prior to activation.\n",
        "        outputs = model(b_input_ids, \n",
        "                             token_type_ids=None, \n",
        "                             attention_mask=b_input_mask, \n",
        "                             labels=b_labels)\n",
        "\n",
        "        loss = outputs[0] \n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "649KP1trONo3",
        "outputId": "4b2ac635-9e40-4b16-d156-ea4537a52bcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======== Epoch 1 / 3 ========\n",
            "Training...\n",
            "  Batch    40  of  3,987.    Elapsed: 0:00:28.\n",
            "  Batch    80  of  3,987.    Elapsed: 0:00:55.\n",
            "  Batch   120  of  3,987.    Elapsed: 0:01:23.\n",
            "  Batch   160  of  3,987.    Elapsed: 0:01:50.\n",
            "  Batch   200  of  3,987.    Elapsed: 0:02:18.\n",
            "  Batch   240  of  3,987.    Elapsed: 0:02:45.\n",
            "  Batch   280  of  3,987.    Elapsed: 0:03:12.\n",
            "  Batch   320  of  3,987.    Elapsed: 0:03:40.\n",
            "  Batch   360  of  3,987.    Elapsed: 0:04:07.\n",
            "  Batch   400  of  3,987.    Elapsed: 0:04:35.\n",
            "  Batch   440  of  3,987.    Elapsed: 0:05:02.\n",
            "  Batch   480  of  3,987.    Elapsed: 0:05:30.\n",
            "  Batch   520  of  3,987.    Elapsed: 0:05:57.\n",
            "  Batch   560  of  3,987.    Elapsed: 0:06:25.\n",
            "  Batch   600  of  3,987.    Elapsed: 0:06:52.\n",
            "  Batch   640  of  3,987.    Elapsed: 0:07:20.\n",
            "  Batch   680  of  3,987.    Elapsed: 0:07:47.\n",
            "  Batch   720  of  3,987.    Elapsed: 0:08:15.\n",
            "  Batch   760  of  3,987.    Elapsed: 0:08:42.\n",
            "  Batch   800  of  3,987.    Elapsed: 0:09:10.\n",
            "  Batch   840  of  3,987.    Elapsed: 0:09:37.\n",
            "  Batch   880  of  3,987.    Elapsed: 0:10:05.\n",
            "  Batch   920  of  3,987.    Elapsed: 0:10:32.\n",
            "  Batch   960  of  3,987.    Elapsed: 0:11:00.\n",
            "  Batch 1,000  of  3,987.    Elapsed: 0:11:27.\n",
            "  Batch 1,040  of  3,987.    Elapsed: 0:11:55.\n",
            "  Batch 1,080  of  3,987.    Elapsed: 0:12:22.\n",
            "  Batch 1,120  of  3,987.    Elapsed: 0:12:50.\n",
            "  Batch 1,160  of  3,987.    Elapsed: 0:13:17.\n",
            "  Batch 1,200  of  3,987.    Elapsed: 0:13:45.\n",
            "  Batch 1,240  of  3,987.    Elapsed: 0:14:12.\n",
            "  Batch 1,280  of  3,987.    Elapsed: 0:14:40.\n",
            "  Batch 1,320  of  3,987.    Elapsed: 0:15:07.\n",
            "  Batch 1,360  of  3,987.    Elapsed: 0:15:35.\n",
            "  Batch 1,400  of  3,987.    Elapsed: 0:16:02.\n",
            "  Batch 1,440  of  3,987.    Elapsed: 0:16:29.\n",
            "  Batch 1,480  of  3,987.    Elapsed: 0:16:57.\n",
            "  Batch 1,520  of  3,987.    Elapsed: 0:17:24.\n",
            "  Batch 1,560  of  3,987.    Elapsed: 0:17:52.\n",
            "  Batch 1,600  of  3,987.    Elapsed: 0:18:19.\n",
            "  Batch 1,640  of  3,987.    Elapsed: 0:18:47.\n",
            "  Batch 1,680  of  3,987.    Elapsed: 0:19:14.\n",
            "  Batch 1,720  of  3,987.    Elapsed: 0:19:42.\n",
            "  Batch 1,760  of  3,987.    Elapsed: 0:20:09.\n",
            "  Batch 1,800  of  3,987.    Elapsed: 0:20:37.\n",
            "  Batch 1,840  of  3,987.    Elapsed: 0:21:04.\n",
            "  Batch 1,880  of  3,987.    Elapsed: 0:21:32.\n",
            "  Batch 1,920  of  3,987.    Elapsed: 0:21:59.\n",
            "  Batch 1,960  of  3,987.    Elapsed: 0:22:27.\n",
            "  Batch 2,000  of  3,987.    Elapsed: 0:22:54.\n",
            "  Batch 2,040  of  3,987.    Elapsed: 0:23:22.\n",
            "  Batch 2,080  of  3,987.    Elapsed: 0:23:49.\n",
            "  Batch 2,120  of  3,987.    Elapsed: 0:24:17.\n",
            "  Batch 2,160  of  3,987.    Elapsed: 0:24:44.\n",
            "  Batch 2,200  of  3,987.    Elapsed: 0:25:12.\n",
            "  Batch 2,240  of  3,987.    Elapsed: 0:25:39.\n",
            "  Batch 2,280  of  3,987.    Elapsed: 0:26:06.\n",
            "  Batch 2,320  of  3,987.    Elapsed: 0:26:34.\n",
            "  Batch 2,360  of  3,987.    Elapsed: 0:27:01.\n",
            "  Batch 2,400  of  3,987.    Elapsed: 0:27:29.\n",
            "  Batch 2,440  of  3,987.    Elapsed: 0:27:56.\n",
            "  Batch 2,480  of  3,987.    Elapsed: 0:28:24.\n",
            "  Batch 2,520  of  3,987.    Elapsed: 0:28:51.\n",
            "  Batch 2,560  of  3,987.    Elapsed: 0:29:19.\n",
            "  Batch 2,600  of  3,987.    Elapsed: 0:29:46.\n",
            "  Batch 2,640  of  3,987.    Elapsed: 0:30:14.\n",
            "  Batch 2,680  of  3,987.    Elapsed: 0:30:41.\n",
            "  Batch 2,720  of  3,987.    Elapsed: 0:31:09.\n",
            "  Batch 2,760  of  3,987.    Elapsed: 0:31:36.\n",
            "  Batch 2,800  of  3,987.    Elapsed: 0:32:04.\n",
            "  Batch 2,840  of  3,987.    Elapsed: 0:32:31.\n",
            "  Batch 2,880  of  3,987.    Elapsed: 0:32:59.\n",
            "  Batch 2,920  of  3,987.    Elapsed: 0:33:26.\n",
            "  Batch 2,960  of  3,987.    Elapsed: 0:33:54.\n",
            "  Batch 3,000  of  3,987.    Elapsed: 0:34:21.\n",
            "  Batch 3,040  of  3,987.    Elapsed: 0:34:49.\n",
            "  Batch 3,080  of  3,987.    Elapsed: 0:35:16.\n",
            "  Batch 3,120  of  3,987.    Elapsed: 0:35:44.\n",
            "  Batch 3,160  of  3,987.    Elapsed: 0:36:11.\n",
            "  Batch 3,200  of  3,987.    Elapsed: 0:36:39.\n",
            "  Batch 3,240  of  3,987.    Elapsed: 0:37:06.\n",
            "  Batch 3,280  of  3,987.    Elapsed: 0:37:34.\n",
            "  Batch 3,320  of  3,987.    Elapsed: 0:38:01.\n",
            "  Batch 3,360  of  3,987.    Elapsed: 0:38:29.\n",
            "  Batch 3,400  of  3,987.    Elapsed: 0:38:56.\n",
            "  Batch 3,440  of  3,987.    Elapsed: 0:39:24.\n",
            "  Batch 3,480  of  3,987.    Elapsed: 0:39:51.\n",
            "  Batch 3,520  of  3,987.    Elapsed: 0:40:19.\n",
            "  Batch 3,560  of  3,987.    Elapsed: 0:40:46.\n",
            "  Batch 3,600  of  3,987.    Elapsed: 0:41:14.\n",
            "  Batch 3,640  of  3,987.    Elapsed: 0:41:41.\n",
            "  Batch 3,680  of  3,987.    Elapsed: 0:42:09.\n",
            "  Batch 3,720  of  3,987.    Elapsed: 0:42:36.\n",
            "  Batch 3,760  of  3,987.    Elapsed: 0:43:03.\n",
            "  Batch 3,800  of  3,987.    Elapsed: 0:43:31.\n",
            "  Batch 3,840  of  3,987.    Elapsed: 0:43:58.\n",
            "  Batch 3,880  of  3,987.    Elapsed: 0:44:26.\n",
            "  Batch 3,920  of  3,987.    Elapsed: 0:44:53.\n",
            "  Batch 3,960  of  3,987.    Elapsed: 0:45:21.\n",
            "\n",
            "======== Epoch 2 / 3 ========\n",
            "Training...\n",
            "  Batch    40  of  3,987.    Elapsed: 0:00:27.\n",
            "  Batch    80  of  3,987.    Elapsed: 0:00:55.\n",
            "  Batch   120  of  3,987.    Elapsed: 0:01:22.\n",
            "  Batch   160  of  3,987.    Elapsed: 0:01:50.\n",
            "  Batch   200  of  3,987.    Elapsed: 0:02:17.\n",
            "  Batch   240  of  3,987.    Elapsed: 0:02:45.\n",
            "  Batch   280  of  3,987.    Elapsed: 0:03:12.\n",
            "  Batch   320  of  3,987.    Elapsed: 0:03:40.\n",
            "  Batch   360  of  3,987.    Elapsed: 0:04:07.\n",
            "  Batch   400  of  3,987.    Elapsed: 0:04:35.\n",
            "  Batch   440  of  3,987.    Elapsed: 0:05:02.\n",
            "  Batch   480  of  3,987.    Elapsed: 0:05:30.\n",
            "  Batch   520  of  3,987.    Elapsed: 0:05:57.\n",
            "  Batch   560  of  3,987.    Elapsed: 0:06:25.\n",
            "  Batch   600  of  3,987.    Elapsed: 0:06:52.\n",
            "  Batch   640  of  3,987.    Elapsed: 0:07:20.\n",
            "  Batch   680  of  3,987.    Elapsed: 0:07:47.\n",
            "  Batch   720  of  3,987.    Elapsed: 0:08:15.\n",
            "  Batch   760  of  3,987.    Elapsed: 0:08:42.\n",
            "  Batch   800  of  3,987.    Elapsed: 0:09:10.\n",
            "  Batch   840  of  3,987.    Elapsed: 0:09:37.\n",
            "  Batch   880  of  3,987.    Elapsed: 0:10:05.\n",
            "  Batch   920  of  3,987.    Elapsed: 0:10:32.\n",
            "  Batch   960  of  3,987.    Elapsed: 0:10:59.\n",
            "  Batch 1,000  of  3,987.    Elapsed: 0:11:27.\n",
            "  Batch 1,040  of  3,987.    Elapsed: 0:11:54.\n",
            "  Batch 1,080  of  3,987.    Elapsed: 0:12:22.\n",
            "  Batch 1,120  of  3,987.    Elapsed: 0:12:49.\n",
            "  Batch 1,160  of  3,987.    Elapsed: 0:13:17.\n",
            "  Batch 1,200  of  3,987.    Elapsed: 0:13:44.\n",
            "  Batch 1,240  of  3,987.    Elapsed: 0:14:12.\n",
            "  Batch 1,280  of  3,987.    Elapsed: 0:14:39.\n",
            "  Batch 1,320  of  3,987.    Elapsed: 0:15:07.\n",
            "  Batch 1,360  of  3,987.    Elapsed: 0:15:34.\n",
            "  Batch 1,400  of  3,987.    Elapsed: 0:16:02.\n",
            "  Batch 1,440  of  3,987.    Elapsed: 0:16:29.\n",
            "  Batch 1,480  of  3,987.    Elapsed: 0:16:57.\n",
            "  Batch 1,520  of  3,987.    Elapsed: 0:17:24.\n",
            "  Batch 1,560  of  3,987.    Elapsed: 0:17:52.\n",
            "  Batch 1,600  of  3,987.    Elapsed: 0:18:19.\n",
            "  Batch 1,640  of  3,987.    Elapsed: 0:18:47.\n",
            "  Batch 1,680  of  3,987.    Elapsed: 0:19:14.\n",
            "  Batch 1,720  of  3,987.    Elapsed: 0:19:42.\n",
            "  Batch 1,760  of  3,987.    Elapsed: 0:20:09.\n",
            "  Batch 1,800  of  3,987.    Elapsed: 0:20:37.\n",
            "  Batch 1,840  of  3,987.    Elapsed: 0:21:04.\n",
            "  Batch 1,880  of  3,987.    Elapsed: 0:21:32.\n",
            "  Batch 1,920  of  3,987.    Elapsed: 0:21:59.\n",
            "  Batch 1,960  of  3,987.    Elapsed: 0:22:27.\n",
            "  Batch 2,000  of  3,987.    Elapsed: 0:22:54.\n",
            "  Batch 2,040  of  3,987.    Elapsed: 0:23:22.\n",
            "  Batch 2,080  of  3,987.    Elapsed: 0:23:49.\n",
            "  Batch 2,120  of  3,987.    Elapsed: 0:24:17.\n",
            "  Batch 2,160  of  3,987.    Elapsed: 0:24:44.\n",
            "  Batch 2,200  of  3,987.    Elapsed: 0:25:11.\n",
            "  Batch 2,240  of  3,987.    Elapsed: 0:25:39.\n",
            "  Batch 2,280  of  3,987.    Elapsed: 0:26:06.\n",
            "  Batch 2,320  of  3,987.    Elapsed: 0:26:34.\n",
            "  Batch 2,360  of  3,987.    Elapsed: 0:27:01.\n",
            "  Batch 2,400  of  3,987.    Elapsed: 0:27:29.\n",
            "  Batch 2,440  of  3,987.    Elapsed: 0:27:56.\n",
            "  Batch 2,480  of  3,987.    Elapsed: 0:28:24.\n",
            "  Batch 2,520  of  3,987.    Elapsed: 0:28:51.\n",
            "  Batch 2,560  of  3,987.    Elapsed: 0:29:19.\n",
            "  Batch 2,600  of  3,987.    Elapsed: 0:29:46.\n",
            "  Batch 2,640  of  3,987.    Elapsed: 0:30:14.\n",
            "  Batch 2,680  of  3,987.    Elapsed: 0:30:41.\n",
            "  Batch 2,720  of  3,987.    Elapsed: 0:31:09.\n",
            "  Batch 2,760  of  3,987.    Elapsed: 0:31:36.\n",
            "  Batch 2,800  of  3,987.    Elapsed: 0:32:04.\n",
            "  Batch 2,840  of  3,987.    Elapsed: 0:32:31.\n",
            "  Batch 2,880  of  3,987.    Elapsed: 0:32:59.\n",
            "  Batch 2,920  of  3,987.    Elapsed: 0:33:26.\n",
            "  Batch 2,960  of  3,987.    Elapsed: 0:33:54.\n",
            "  Batch 3,000  of  3,987.    Elapsed: 0:34:21.\n",
            "  Batch 3,040  of  3,987.    Elapsed: 0:34:49.\n",
            "  Batch 3,080  of  3,987.    Elapsed: 0:35:16.\n",
            "  Batch 3,120  of  3,987.    Elapsed: 0:35:44.\n",
            "  Batch 3,160  of  3,987.    Elapsed: 0:36:11.\n",
            "  Batch 3,200  of  3,987.    Elapsed: 0:36:39.\n",
            "  Batch 3,240  of  3,987.    Elapsed: 0:37:06.\n",
            "  Batch 3,280  of  3,987.    Elapsed: 0:37:34.\n",
            "  Batch 3,320  of  3,987.    Elapsed: 0:38:01.\n",
            "  Batch 3,360  of  3,987.    Elapsed: 0:38:29.\n",
            "  Batch 3,400  of  3,987.    Elapsed: 0:38:56.\n",
            "  Batch 3,440  of  3,987.    Elapsed: 0:39:24.\n",
            "  Batch 3,480  of  3,987.    Elapsed: 0:39:51.\n",
            "  Batch 3,520  of  3,987.    Elapsed: 0:40:19.\n",
            "  Batch 3,560  of  3,987.    Elapsed: 0:40:46.\n",
            "  Batch 3,600  of  3,987.    Elapsed: 0:41:13.\n",
            "  Batch 3,640  of  3,987.    Elapsed: 0:41:41.\n",
            "  Batch 3,680  of  3,987.    Elapsed: 0:42:08.\n",
            "  Batch 3,720  of  3,987.    Elapsed: 0:42:36.\n",
            "  Batch 3,760  of  3,987.    Elapsed: 0:43:03.\n",
            "  Batch 3,800  of  3,987.    Elapsed: 0:43:31.\n",
            "  Batch 3,840  of  3,987.    Elapsed: 0:43:58.\n",
            "  Batch 3,880  of  3,987.    Elapsed: 0:44:26.\n",
            "  Batch 3,920  of  3,987.    Elapsed: 0:44:53.\n",
            "  Batch 3,960  of  3,987.    Elapsed: 0:45:21.\n",
            "\n",
            "======== Epoch 3 / 3 ========\n",
            "Training...\n",
            "  Batch    40  of  3,987.    Elapsed: 0:00:27.\n",
            "  Batch    80  of  3,987.    Elapsed: 0:00:55.\n",
            "  Batch   120  of  3,987.    Elapsed: 0:01:22.\n",
            "  Batch   160  of  3,987.    Elapsed: 0:01:50.\n",
            "  Batch   200  of  3,987.    Elapsed: 0:02:17.\n",
            "  Batch   240  of  3,987.    Elapsed: 0:02:45.\n",
            "  Batch   280  of  3,987.    Elapsed: 0:03:12.\n",
            "  Batch   320  of  3,987.    Elapsed: 0:03:40.\n",
            "  Batch   360  of  3,987.    Elapsed: 0:04:07.\n",
            "  Batch   400  of  3,987.    Elapsed: 0:04:35.\n",
            "  Batch   440  of  3,987.    Elapsed: 0:05:02.\n",
            "  Batch   480  of  3,987.    Elapsed: 0:05:30.\n",
            "  Batch   520  of  3,987.    Elapsed: 0:05:57.\n",
            "  Batch   560  of  3,987.    Elapsed: 0:06:25.\n",
            "  Batch   600  of  3,987.    Elapsed: 0:06:52.\n",
            "  Batch   640  of  3,987.    Elapsed: 0:07:20.\n",
            "  Batch   680  of  3,987.    Elapsed: 0:07:47.\n",
            "  Batch   720  of  3,987.    Elapsed: 0:08:15.\n",
            "  Batch   760  of  3,987.    Elapsed: 0:08:42.\n",
            "  Batch   800  of  3,987.    Elapsed: 0:09:10.\n",
            "  Batch   840  of  3,987.    Elapsed: 0:09:37.\n",
            "  Batch   880  of  3,987.    Elapsed: 0:10:05.\n",
            "  Batch   920  of  3,987.    Elapsed: 0:10:32.\n",
            "  Batch   960  of  3,987.    Elapsed: 0:11:00.\n",
            "  Batch 1,000  of  3,987.    Elapsed: 0:11:27.\n",
            "  Batch 1,040  of  3,987.    Elapsed: 0:11:54.\n",
            "  Batch 1,080  of  3,987.    Elapsed: 0:12:22.\n",
            "  Batch 1,120  of  3,987.    Elapsed: 0:12:49.\n",
            "  Batch 1,160  of  3,987.    Elapsed: 0:13:17.\n",
            "  Batch 1,200  of  3,987.    Elapsed: 0:13:44.\n",
            "  Batch 1,240  of  3,987.    Elapsed: 0:14:12.\n",
            "  Batch 1,280  of  3,987.    Elapsed: 0:14:39.\n",
            "  Batch 1,320  of  3,987.    Elapsed: 0:15:07.\n",
            "  Batch 1,360  of  3,987.    Elapsed: 0:15:34.\n",
            "  Batch 1,400  of  3,987.    Elapsed: 0:16:02.\n",
            "  Batch 1,440  of  3,987.    Elapsed: 0:16:29.\n",
            "  Batch 1,480  of  3,987.    Elapsed: 0:16:57.\n",
            "  Batch 1,520  of  3,987.    Elapsed: 0:17:24.\n",
            "  Batch 1,560  of  3,987.    Elapsed: 0:17:52.\n",
            "  Batch 1,600  of  3,987.    Elapsed: 0:18:19.\n",
            "  Batch 1,640  of  3,987.    Elapsed: 0:18:47.\n",
            "  Batch 1,680  of  3,987.    Elapsed: 0:19:14.\n",
            "  Batch 1,720  of  3,987.    Elapsed: 0:19:42.\n",
            "  Batch 1,760  of  3,987.    Elapsed: 0:20:09.\n",
            "  Batch 1,800  of  3,987.    Elapsed: 0:20:37.\n",
            "  Batch 1,840  of  3,987.    Elapsed: 0:21:04.\n",
            "  Batch 1,880  of  3,987.    Elapsed: 0:21:32.\n",
            "  Batch 1,920  of  3,987.    Elapsed: 0:21:59.\n",
            "  Batch 1,960  of  3,987.    Elapsed: 0:22:27.\n",
            "  Batch 2,000  of  3,987.    Elapsed: 0:22:54.\n",
            "  Batch 2,040  of  3,987.    Elapsed: 0:23:21.\n",
            "  Batch 2,080  of  3,987.    Elapsed: 0:23:49.\n",
            "  Batch 2,120  of  3,987.    Elapsed: 0:24:16.\n",
            "  Batch 2,160  of  3,987.    Elapsed: 0:24:44.\n",
            "  Batch 2,200  of  3,987.    Elapsed: 0:25:11.\n",
            "  Batch 2,240  of  3,987.    Elapsed: 0:25:39.\n",
            "  Batch 2,280  of  3,987.    Elapsed: 0:26:06.\n",
            "  Batch 2,320  of  3,987.    Elapsed: 0:26:34.\n",
            "  Batch 2,360  of  3,987.    Elapsed: 0:27:01.\n",
            "  Batch 2,400  of  3,987.    Elapsed: 0:27:29.\n",
            "  Batch 2,440  of  3,987.    Elapsed: 0:27:56.\n",
            "  Batch 2,480  of  3,987.    Elapsed: 0:28:24.\n",
            "  Batch 2,520  of  3,987.    Elapsed: 0:28:51.\n",
            "  Batch 2,560  of  3,987.    Elapsed: 0:29:19.\n",
            "  Batch 2,600  of  3,987.    Elapsed: 0:29:46.\n",
            "  Batch 2,640  of  3,987.    Elapsed: 0:30:14.\n",
            "  Batch 2,680  of  3,987.    Elapsed: 0:30:41.\n",
            "  Batch 2,720  of  3,987.    Elapsed: 0:31:09.\n",
            "  Batch 2,760  of  3,987.    Elapsed: 0:31:36.\n",
            "  Batch 2,800  of  3,987.    Elapsed: 0:32:04.\n",
            "  Batch 2,840  of  3,987.    Elapsed: 0:32:31.\n",
            "  Batch 2,880  of  3,987.    Elapsed: 0:32:59.\n",
            "  Batch 2,920  of  3,987.    Elapsed: 0:33:26.\n",
            "  Batch 2,960  of  3,987.    Elapsed: 0:33:54.\n",
            "  Batch 3,000  of  3,987.    Elapsed: 0:34:21.\n",
            "  Batch 3,040  of  3,987.    Elapsed: 0:34:49.\n",
            "  Batch 3,080  of  3,987.    Elapsed: 0:35:16.\n",
            "  Batch 3,120  of  3,987.    Elapsed: 0:35:44.\n",
            "  Batch 3,160  of  3,987.    Elapsed: 0:36:11.\n",
            "  Batch 3,200  of  3,987.    Elapsed: 0:36:39.\n",
            "  Batch 3,240  of  3,987.    Elapsed: 0:37:06.\n",
            "  Batch 3,280  of  3,987.    Elapsed: 0:37:33.\n",
            "  Batch 3,320  of  3,987.    Elapsed: 0:38:01.\n",
            "  Batch 3,360  of  3,987.    Elapsed: 0:38:28.\n",
            "  Batch 3,400  of  3,987.    Elapsed: 0:38:56.\n",
            "  Batch 3,440  of  3,987.    Elapsed: 0:39:23.\n",
            "  Batch 3,480  of  3,987.    Elapsed: 0:39:51.\n",
            "  Batch 3,520  of  3,987.    Elapsed: 0:40:18.\n",
            "  Batch 3,560  of  3,987.    Elapsed: 0:40:46.\n",
            "  Batch 3,600  of  3,987.    Elapsed: 0:41:13.\n",
            "  Batch 3,640  of  3,987.    Elapsed: 0:41:41.\n",
            "  Batch 3,680  of  3,987.    Elapsed: 0:42:08.\n",
            "  Batch 3,720  of  3,987.    Elapsed: 0:42:36.\n",
            "  Batch 3,760  of  3,987.    Elapsed: 0:43:03.\n",
            "  Batch 3,800  of  3,987.    Elapsed: 0:43:31.\n",
            "  Batch 3,840  of  3,987.    Elapsed: 0:43:58.\n",
            "  Batch 3,880  of  3,987.    Elapsed: 0:44:26.\n",
            "  Batch 3,920  of  3,987.    Elapsed: 0:44:53.\n",
            "  Batch 3,960  of  3,987.    Elapsed: 0:45:21.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Average training loss"
      ],
      "metadata": {
        "id": "ky7olONdvGaC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "avg_train_loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24S1_8cr4u6p",
        "outputId": "678e501e-7361-4961-c509-1a06feeabd64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.07614396950366653"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validation "
      ],
      "metadata": {
        "id": "Xcu3TWdkvKId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\")\n",
        "print(\"Running Validation...\")\n",
        "\n",
        "t0 = time.time()\n",
        "   # Put the model in evaluation mode--the dropout layers behave differently\n",
        "  # during evaluation.\n",
        "model.eval()\n",
        "   # Tracking variables \n",
        "total_eval_accuracy = 0\n",
        "total_eval_loss = 0\n",
        "nb_eval_steps = 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "for batch in validation_dataloader:\n",
        "        \n",
        "      # Unpack this training batch from our dataloader. \n",
        "      #\n",
        "      # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
        "      # the `to` method.\n",
        "      #\n",
        "      # `batch` contains three pytorch tensors:\n",
        "      #   [0]: input ids \n",
        "      #   [1]: attention masks\n",
        "      #   [2]: labels \n",
        "    b_input_ids = batch[0].to(device)\n",
        "    b_input_mask = batch[1].to(device)\n",
        "    b_labels = batch[2].to(device)\n",
        "       \n",
        "    with torch.no_grad():        \n",
        "          outputs = model(b_input_ids, \n",
        "                               token_type_ids=None, \n",
        "                               attention_mask=b_input_mask,\n",
        "                               labels=b_labels)\n",
        "          \n",
        "      # Accumulate the validation loss.\n",
        "    loss = outputs[0]\n",
        "    total_eval_loss += loss.item()\n",
        "    logits = outputs[1]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch of test sentences, and\n",
        "        # accumulate it over all batches.\n",
        "    total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "      \n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "    \n",
        "    # Measure how long the validation run took.\n",
        "validation_time = format_time(time.time() - t0)\n",
        "    \n",
        "print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "    # Record all statistics from this epoch.\n",
        "training_stats.append(\n",
        "    {\n",
        "        'epoch': epoch_i + 1,\n",
        "        'Training Loss': avg_train_loss,\n",
        "        'Valid. Loss': avg_val_loss,\n",
        "        'Valid. Accur.': avg_val_accuracy,\n",
        "        'Training Time': training_time,\n",
        "        'Validation Time': validation_time\n",
        "    }\n",
        ")\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
        "     "
      ],
      "metadata": {
        "id": "Iqux71C3KFcz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ef9a8b7-16ea-48ea-ea2e-7da5b6037535"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.96\n",
            "  Validation Loss: 0.11\n",
            "  Validation took: 0:03:12\n",
            "\n",
            "Training complete!\n",
            "Total training took 2:21:57 (h:mm:ss)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tabular display of time and losses (Time is per epochs)"
      ],
      "metadata": {
        "id": "4nKgTXUfvOT5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Display floats with two decimal places.\n",
        "pd.set_option('precision', 2)\n",
        "\n",
        "# Create a DataFrame from our training statistics.\n",
        "df_stats = pd.DataFrame(data=training_stats)\n",
        "\n",
        "# Use the 'epoch' as the row index.\n",
        "df_stats = df_stats.set_index('epoch')\n",
        "\n",
        "# A hack to force the column headers to wrap.\n",
        "#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
        "\n",
        "# Display the table.\n",
        "df_stats"
      ],
      "metadata": {
        "id": "9ySmpiewKv7t",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "outputId": "be8e429e-73af-4b71-a7bc-c3b494f0ed42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
              "epoch                                                                         \n",
              "3               0.08         0.11           0.96       0:45:39         0:03:12"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5db642a9-259c-4c08-a7b9-771fd53b529b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Valid. Loss</th>\n",
              "      <th>Valid. Accur.</th>\n",
              "      <th>Training Time</th>\n",
              "      <th>Validation Time</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>epoch</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.08</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.96</td>\n",
              "      <td>0:45:39</td>\n",
              "      <td>0:03:12</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5db642a9-259c-4c08-a7b9-771fd53b529b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5db642a9-259c-4c08-a7b9-771fd53b529b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5db642a9-259c-4c08-a7b9-771fd53b529b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving and loading entire dataset (not train val split)"
      ],
      "metadata": {
        "id": "WG3E_wNbgNOb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saving"
      ],
      "metadata": {
        "id": "ZHLhqU8ovmtx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
        "\n",
        "output_dir = '/content/drive/My Drive/sentiment_datasets/xlm-roberta_model_save'\n",
        "\n",
        "# Create output directory if needed\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "# They can then be reloaded using `from_pretrained()`\n",
        "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "model_to_save.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "# Good practice: save your training arguments together with the trained model\n",
        "# torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4w6K-yCf5k-e",
        "outputId": "2eddee64-1a2f-49f4-a9e0-47cc379dbe45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving model to /content/drive/My Drive/sentiment_datasets/xlm-roberta_model_save\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/My Drive/sentiment_datasets/xlm-roberta_model_save/tokenizer_config.json',\n",
              " '/content/drive/My Drive/sentiment_datasets/xlm-roberta_model_save/special_tokens_map.json',\n",
              " '/content/drive/My Drive/sentiment_datasets/xlm-roberta_model_save/sentencepiece.bpe.model',\n",
              " '/content/drive/My Drive/sentiment_datasets/xlm-roberta_model_save/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "\n",
        "from transformers import XLMRobertaForSequenceClassification\n",
        "\n",
        "output_dir = '/content/drive/My Drive/sentiment_datasets/xlm-roberta_model_save'\n",
        "\n",
        "print(output_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDgJTo-a5rUl",
        "outputId": "a4473516-1314-4d51-879f-1155072c4df7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.26.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.26.14)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "/content/drive/My Drive/sentiment_datasets/xlm-roberta_model_save\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading"
      ],
      "metadata": {
        "id": "zFMw3GpGvrU3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import XLMRobertaTokenizer\n",
        "import torch\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading XLMRobertaTokenizer...')\n",
        "tokenizer = XLMRobertaTokenizer.from_pretrained(output_dir)\n",
        "model_loaded = XLMRobertaForSequenceClassification.from_pretrained(output_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3F4TuSA5u6v",
        "outputId": "c73ba9c8-8da3-4665-915e-0be5707095a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading XLMRobertaTokenizer...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's check it for a given sentence\n",
        "\n",
        "# Hindi\n",
        "hindi_negative_sent = \"   , ,      , , , , ,, \"\n",
        "hindi_negative_sentence=\" ,  ,  ,     ,    , ,,,        \"\n",
        "\n",
        "# Marathi\n",
        "marathi_positive=\"  , \"\n",
        "marathi_negative=\",   , \"\n",
        "\n",
        "# Arabic\n",
        "arabic_negative = \"         \"\n",
        "arabic_positive=\"       .   .\"\n",
        "\n",
        "# Gujarati\n",
        "gujarati_negative =\"    ,        ,  ,    ,               \"\n",
        "gujarati_positive = \" ,    \"\n",
        "\n",
        "# Mix of english and marathi\n",
        "engarati_negative = \"You are so beautiful my princess, I love you so much. ,   ,\"\n",
        "\n",
        "\n",
        "encoded_dict = tokenizer.encode_plus(\n",
        "                        marathi_positive,          # Sentence for embeddings.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 64,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "input_id = encoded_dict['input_ids']\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "attention_mask = encoded_dict['attention_mask']\n",
        "input_id = torch.LongTensor(input_id)\n",
        "attention_mask = torch.LongTensor(attention_mask)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HZC9tN05xPE",
        "outputId": "72f17e9b-1a37-4c7b-9f91-a4bf4a22fd08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading model and others to gpu if available or CPU"
      ],
      "metadata": {
        "id": "nSGtjUzowKB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_loaded = model_loaded.to(device)\n",
        "input_id = input_id.to(device)\n",
        "attention_mask = attention_mask.to(device)"
      ],
      "metadata": {
        "id": "ElmKuo-R7QOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {
        "id": "g6YOasCswQek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  # Forward pass, calculate logit predictions\n",
        "  outputs = model_loaded(input_id, token_type_ids=None, attention_mask=attention_mask)\n",
        "# print(outputs[0])\n",
        "logits = outputs[0]\n",
        "# calculate the softmax of a vector\n",
        "def softmax(vector):\n",
        " e = np.exp(vector)\n",
        " return e / e.sum()\n",
        "\n",
        "prob = softmax(logits.to('cpu'))\n",
        "index = logits.argmax()\n",
        "# print(prob)\n",
        "\n",
        "if(prob[0][0]>prob[0][1]):\n",
        "  print(\"Non-Toxic\")\n",
        "else:\n",
        "  print(\"Toxic\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMeZES2i52ky",
        "outputId": "4d303728-3574-4adb-8eb6-618b173c4372"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Non-Toxic\n"
          ]
        }
      ]
    }
  ]
}